{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Exploring_Pub_Finder_v2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1ji0gcIIlqqZ-BKOTIZGVp2o6kW49aNe6",
      "authorship_tag": "ABX9TyObIPDbefaG/Wdp5B5Hcbda",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/5374m164a1l/CSE5045-Project/blob/master/Exploring_Pub_Finder_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w26HKw122SB0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -q transformers\n",
        "!pip install -q bert-tensorflow\n",
        "#!pip install -q pyyaml h5py  # Required to save models in HDF5 format\n",
        "!mkdir /content/training_1\n",
        "%cd /content/training_1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cI12clwLCNWi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!cat /usr/local/lib/python3.6/dist-packages/transformers/configuration_bert.py\n",
        "#!cat /usr/local/lib/python3.6/dist-packages/bert/tokenization.py\n",
        "#!cat /usr/local/lib/python3.6/dist-packages/tokenizers/implementations/bert_wordpiece.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0itNDet2o_m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import os, re\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "tqdm.pandas()\n",
        "\n",
        "import transformers\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import bert\n",
        "from bert import tokenization\n",
        "\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, CSVLogger\n",
        "\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tokenizers import BertWordPieceTokenizer\n",
        "from tensorflow.keras.layers import Dense, Input, Dropout, Embedding\n",
        "from tensorflow.keras.layers import LSTM, GRU, Conv1D, SpatialDropout1D\n",
        "\n",
        "#from tensorflow.keras import layers\n",
        "#from tensorflow.keras import optimizers\n",
        "#from tensorflow.keras import activations\n",
        "#from tensorflow.keras import constraints\n",
        "#from tensorflow.keras import initializers\n",
        "#from tensorflow.keras import regularizers\n",
        "\n",
        "checkpoint_path = \"training_1/cp.ckpt\"\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "np.random.seed(0)\n",
        "\n",
        "print('\\ndone')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54k6HVvB2v24",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATA_PATH = \"/content/drive/My Drive/pf_dsets/pf_subset\"\n",
        "NUMBER = \"1\"\n",
        "PROC = \"-processed-seqlen256.csv\"\n",
        "#PROC = \"\"\n",
        "SOURCE_PATH = DATA_PATH + NUMBER + PROC\n",
        "#VAL_PATH = DATA_PATH + \"validation\"+ PROC +\".csv\"\n",
        "#TRAIN_PATH = DATA_PATH + \"jigsaw-toxic-comment-train\"+ PROC +\".csv\"\n",
        "\n",
        "EPOCHS = 3\n",
        "BATCH_SIZE = 16\n",
        "#N_STEPS = 500\n",
        "N_STEPS = 50\n",
        "SEQUENCE_LENGTH = 256\n",
        "\n",
        "pf_d1 = pd.read_csv(SOURCE_PATH)\n",
        "#val_data = pd.read_csv(VAL_PATH)\n",
        "#test_data = pd.read_csv(TEST_PATH)\n",
        "#train_data = pd.read_csv(TRAIN_PATH)\n",
        "\n",
        "\n",
        "print('\\ndone')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8mAv8asgkY-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def str2int(line):\n",
        "  line = line.replace('(','')\n",
        "  line = line.replace(')','')\n",
        "  line = line.replace(',',' ')\n",
        "  narray = line.split()\n",
        "  return list(map(int, narray))\n",
        "def convert(frame):\n",
        "  lol = []\n",
        "  for i in range(frame.shape[0]):\n",
        "    lol.append(str2int(frame.iloc[i,3]))\n",
        "  return np.asarray(lol,dtype=np.int32)\n",
        "print('\\ndone')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "paXRRgCu96wx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_tokenizer():#bert_path=BERT_PATH_SAVEDMODEL):\n",
        "    \"\"\"Get the tokenizer for a BERT layer.\"\"\"\n",
        "    #bert_path = \"https://tfhub.dev/google/small_bert/bert_uncased_L-2_H-256_A-4/1\"\n",
        "    #bert_path = \"https://tfhub.dev/tensorflow/bert_en_cased_L-24_H-1024_A-16/2\"\n",
        "    bert_path = \"https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/2\"\n",
        "    bert_model = hub.keras_layer.load_module(bert_path)\n",
        "    \n",
        "    vocab_file = bert_model.vocab_file.asset_path.numpy()\n",
        "    cased = bert_model.do_lower_case.numpy()\n",
        "    tf.gfile = tf.io.gfile  # for bert.tokenization.load_vocab in tokenizer\n",
        "    tokenizer = bert.tokenization.FullTokenizer(vocab_file, cased)\n",
        "    #tokenizer = BertWordPieceTokenizer(vocab_file,lowercase=False)\n",
        "  \n",
        "    return tokenizer\n",
        "\n",
        "tokenizer = get_tokenizer()\n",
        "\n",
        "def process_sentence(sentence, max_seq_length=SEQUENCE_LENGTH, tokenizer=tokenizer):\n",
        "    \"\"\" Converts sentence to ['input_word_ids', 'input_mask', 'segment_ids'] for BERT. \"\"\"\n",
        "    # Tokenize, and truncate to max_seq_length if necessary.\n",
        "    tokens = tokenizer.tokenize(sentence)\n",
        "    if len(tokens) > max_seq_length - 2:\n",
        "        tokens = tokens[:(max_seq_length - 2)]\n",
        "\n",
        "    # Convert the tokens in the sentence to word IDs.\n",
        "    input_ids = tokenizer.convert_tokens_to_ids([\"[CLS]\"] + tokens + [\"[SEP]\"])\n",
        "\n",
        "    # The mask has 1 for real tokens and 0 for padding tokens. Only real tokens are attended to.\n",
        "    input_mask = [1] * len(input_ids)\n",
        "\n",
        "    # Zero-pad up to the sequence length.\n",
        "    pad_length = max_seq_length - len(input_ids)\n",
        "    input_ids.extend([0] * pad_length)\n",
        "    input_mask.extend([0] * pad_length)\n",
        "\n",
        "    # We only have one input segment.\n",
        "    segment_ids = [0] * max_seq_length\n",
        "\n",
        "    return (input_ids, input_mask, segment_ids)\n",
        "print('\\ndone')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sekwgx8I2xqY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "AUTO = tf.data.experimental.AUTOTUNE\n",
        "\n",
        "test_data = pf_d1.iloc[1:500,:]\n",
        "val_data = pf_d1.iloc[500:2500,:]\n",
        "train_data = pf_d1.iloc[2500:,:]\n",
        "\n",
        "x_train = convert(train_data)\n",
        "x_valid = convert(val_data)\n",
        "x_test = convert(test_data)\n",
        "\n",
        "#length of list equals number of samples, depth equals number of classes, result is shape (samples,classes)\n",
        "y_valid = np.asarray(tf.one_hot(list(val_data.pubID.values),\n",
        "                     depth=1370,\n",
        "                     axis=-1,\n",
        "                     dtype=tf.int32))\n",
        "\n",
        "#y_valid = val_data.pubID.values\n",
        "y_train = np.asarray(tf.one_hot(list(train_data.pubID.values),\n",
        "                     depth=1370,\n",
        "                     axis=-1,\n",
        "                     dtype=tf.int32))\n",
        "#need y_test for evaluate\n",
        "y_test = np.asarray(tf.one_hot(list(test_data.pubID.values),\n",
        "                     depth=1370,\n",
        "                     axis=-1,\n",
        "                     dtype=tf.int32))\n",
        "\n",
        "train_dataset = (\n",
        "    tf.data.Dataset\n",
        "    .from_tensor_slices((x_train, y_train))\n",
        "    .repeat()\n",
        "    .shuffle(2048)\n",
        "    .batch(BATCH_SIZE)\n",
        "    .prefetch(AUTO)\n",
        ")\n",
        "valid_dataset = (\n",
        "    tf.data.Dataset\n",
        "    .from_tensor_slices((x_valid, y_valid))\n",
        "    .batch(BATCH_SIZE)\n",
        "    .cache()\n",
        "    .prefetch(AUTO)\n",
        ")\n",
        "\n",
        "test_dataset = (\n",
        "    tf.data.Dataset\n",
        "    .from_tensor_slices(x_test)\n",
        "    .batch(BATCH_SIZE)\n",
        ")\n",
        "\n",
        "print('\\ndone')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6aRdvIFv5r3p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#important info here\n",
        "def build_model(transformer, max_len=256):\n",
        "    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
        "    #something may be wrong here\n",
        "    sequence_output = transformer(input_word_ids)[0]\n",
        "    cls_token = sequence_output[:, 0, :]\n",
        "    cls_token = Dense(512, activation=\"hard_sigmoid\")(cls_token)\n",
        "    cls_token = Dropout(0.12)(cls_token)\n",
        "    out = Dense(1370, activation='softmax')(cls_token)#can maybe try hard_sigmoid\n",
        "    \n",
        "    model = Model(inputs=input_word_ids, outputs=out)\n",
        "    \n",
        "    \n",
        "    model.compile(Adam(lr=1.5e-1), #try 1.5e-2, originally 1.5e-5\n",
        "                  loss='categorical_crossentropy', \n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        "    return model\n",
        "print('\\ndone')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCu0aWu56L_9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a callback that saves the model's weights\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
        "                                                 save_weights_only=True,\n",
        "                                                 verbose=1)\n",
        "#many models to try here\n",
        "transformer_layer = transformers.TFBertModel.\\\n",
        "    from_pretrained('bert-base-cased')\n",
        "bert_model = build_model(transformer_layer, max_len=256)\n",
        "bert_model.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPjush_nnnNP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#need this whenever a change is made to the model\n",
        "#directory is empty for first time run. To save weights, move the checkpoints off colab\n",
        "bert_model.load_weights(checkpoint_path)\n",
        "print('\\ndone')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bt3-FJTJOfZY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#bert_model.load_weights(checkpoint_path)\n",
        "#bert_model = tf.saved_model.load()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NO7h0-6f6TCV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "################--Run the model--################\n",
        "train_history = bert_model.fit(\n",
        "    train_dataset,\n",
        "    steps_per_epoch=N_STEPS,\n",
        "    validation_data=valid_dataset,\n",
        "    callbacks=[cp_callback],\n",
        "    epochs=EPOCHS\n",
        ")\n",
        "#need a way to have secondary loss (i.e. if correct label is not equal max of output but is in top 5 or 10, then scale down error)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXuipFpACyqg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#we need only one configuration\n",
        "#tf.saved_model.save(bert_model, checkpoint_dir)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byt4x9J-fEV5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "re_model = tf.saved_model.load(checkpoint_dir)\n",
        "re_model.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E15-eXgezNm1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "#plotting\n",
        "history_dict = train_history.history\n",
        "history_dict.keys()\n",
        "\n",
        "acc = history_dict['accuracy']\n",
        "val_acc = history_dict['val_accuracy']\n",
        "loss = history_dict['loss']\n",
        "val_loss = history_dict['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "# \"bo\" is for \"blue dot\"\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "# b is for \"solid blue line\"\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "plt.clf()   # clear figure\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYD_6jkw9Wzb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "original_text = pf_d1.iloc[23137,1]\n",
        "original_pubid = pf_d1.iloc[23137,0]\n",
        "#original_text = \"\"\n",
        "new_word_vec = np.asarray(process_sentence(original_text)[0])\n",
        "cartridge = (\n",
        "    tf.data.Dataset\n",
        "    .from_tensor_slices(new_word_vec.reshape(1,-1))\n",
        "    .batch(1)\n",
        "    #.prefetch(AUTO)\n",
        ")\n",
        "print(\"specs of cartridge:\",cartridge.element_spec)\n",
        "print(cartridge)\n",
        "\n",
        "prediction = bert_model.predict(cartridge)\n",
        "prediction_copy = prediction.copy()\n",
        "print(\"prediction shape:\",prediction.shape)\n",
        "print(np.argmax(prediction_copy),np.argmax(prediction))\n",
        "\n",
        "top5 = np.zeros((1,5),dtype=np.int16)\n",
        "for i in range(5):\n",
        "  idx = np.argmax(prediction)\n",
        "  top5[0,i] = idx\n",
        "  prediction[0,idx] = 0\n",
        "print(\"correct;\",original_pubid,\"predicted:\",top5)\n",
        "print('\\ndone')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUJECGuaXKcz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def bestof(n):\n",
        "  topn = np.zeros((len(test_dataset),n),dtype=np.int16)\n",
        "  out = bert_model.predict(test_dataset)\n",
        "  for i in range(topn.shape[0]):\n",
        "    for j in range(n):\n",
        "      topn[i,j] = np.argmax(out[i,:])\n",
        "      out[i,topn[i,j]] = 0\n",
        "  return topn\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtajtguFRUuS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#bert_pred = bert_model.make_predict_function()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PcgPB__PCbcg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lookup = pd.read_csv('/content/drive/My Drive/pf_dsets/pf_lookup.csv')\n",
        "print(\"Best match first:\\n\")\n",
        "for i in range(5):\n",
        "  print(lookup.iloc[top5[0,i],0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03-n3gNOMyG2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.python.client import device_lib\n",
        "print(device_lib.list_local_devices())"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}